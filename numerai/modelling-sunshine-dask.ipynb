{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1124b60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "from IPython.display import display, HTML, clear_output\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8f541acf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "TO_TRAIN = True\n",
    "RUN_NAME = \"single_target_medm_feats_every4th_int8\"\n",
    "TO_DOWNLOAD_DATA = True\n",
    "TRAINING_PARAMS = {\n",
    "    \"neutralisation\": 0.5,\n",
    "    \"sample_every4\": True,\n",
    "}\n",
    "# small fast params\n",
    "params_name = \"sm_lgbm\"\n",
    "params = {\"n_estimators\": 5000,\n",
    "          \"learning_rate\": 0.001,\n",
    "          \"max_depth\": 6,\n",
    "          \"num_leaves\": 2 ** 6,\n",
    "          \"colsample_bytree\": 0.1}\n",
    "\n",
    "# recommended params\n",
    "# params_name = \"lg_lgbm\"\n",
    "# params = {\n",
    "#     \"n_estimators\": 20000,\n",
    "#     \"learning_rate\": 0.001,\n",
    "#     \"max_depth\": 6,\n",
    "#     \"num_leaves\": 2**6,\n",
    "#     \"colsample_bytree\": 0.1,\n",
    "# }\n",
    "\n",
    "# loop through all of our favorite targets and build models on each of them - one over training data, one over all available data\n",
    "# for the train_data models, we'll then predict on validation data\n",
    "# for the all_data models, we'll predict on live\n",
    "targets = [\n",
    "    \"target_nomi_v4_20\",\n",
    "#     \"target_jerome_v4_60\",\n",
    "#     \"target_ralph_v4_20\",\n",
    "#     \"target_tyler_v4_20\",\n",
    "#     \"target_victor_v4_20\",\n",
    "#     \"target_waldo_v4_20\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98fa49a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "from lightgbm import LGBMRegressor\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "import gc\n",
    "import json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import os.path\n",
    "\n",
    "from numerapi import NumerAPI\n",
    "import mlflow\n",
    "from utils import (\n",
    "    save_model,\n",
    "    load_model,\n",
    "    neutralize,\n",
    "    validation_metrics,\n",
    "    ERA_COL,\n",
    "    DATA_TYPE_COL,\n",
    "    TARGET_COL,\n",
    "    EXAMPLE_PREDS_COL,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd1e1865",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/vispers/work/numerai/numerai'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edba5bd2",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26da90a2",
   "metadata": {},
   "source": [
    "### 1. Download relevant datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c210ae22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "feature_set_name = \"medium\"\n",
    "data_fld_root = \"../data\"\n",
    "dataset_name = \"v4.1\"\n",
    "data_path = os.path.join(data_fld_root, dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c376477e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current round: 458\n",
      "Downloading dataset files...\n",
      "train_int8.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-08 08:44:53,377 INFO numerapi.utils: target file already exists\n",
      "2023-04-08 08:44:53,380 INFO numerapi.utils: download complete\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_int8.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-08 08:44:54,809 INFO numerapi.utils: target file already exists\n",
      "2023-04-08 08:44:54,812 INFO numerapi.utils: download complete\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-08 08:44:56,236 INFO numerapi.utils: target file already exists\n",
      "2023-04-08 08:44:56,238 INFO numerapi.utils: download complete\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_example_preds.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-08 08:44:57,730 INFO numerapi.utils: target file already exists\n",
      "2023-04-08 08:44:57,732 INFO numerapi.utils: download complete\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-08 08:44:59,068 INFO numerapi.utils: target file already exists\n",
      "2023-04-08 08:44:59,071 INFO numerapi.utils: download complete\n",
      "2023-04-08 08:45:00,672 INFO numerapi.utils: target file already exists\n",
      "2023-04-08 08:45:00,673 INFO numerapi.utils: download complete\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "napi = NumerAPI()\n",
    "current_round = napi.get_current_round()\n",
    "print(f\"Current round: {current_round}\")\n",
    "\n",
    "if not TO_DOWNLOAD_DATA:\n",
    "    print(\"Not downloading data; assuming it exists already\")\n",
    "else:\n",
    "    # Tournament data changes every week so we specify the round in their name. Training\n",
    "    # and validation data only change periodically, so no need to download them every time.\n",
    "    print(\"Downloading dataset files...\")\n",
    "    # we'll use the int8 in this example in order to save RAM.\n",
    "    # if you remove the int8 suffix for each of these files, you'll get features between 0 and 1 as floats.\n",
    "    # int_8 files are much smaller...\n",
    "    # but are harder to work with because some packages don't like ints and the way NAs are encoded.\n",
    "\n",
    "    # napi.download_dataset(f\"{dataset_name}/train.parquet\")\n",
    "    # napi.download_dataset(f\"{dataset_name}/validation.parquet\")\n",
    "    # napi.download_dataset(f\"{dataset_name}/live.parquet\", f\"{dataset_name}/live_{current_round}.parquet\")\n",
    "\n",
    "    for fl in [\n",
    "        \"train_int8.parquet\",\n",
    "        \"validation_int8.parquet\", \n",
    "        \"features.json\",\n",
    "        \"validation_example_preds.parquet\",\n",
    "        \"features.json\",\n",
    "    ]:\n",
    "        print(f\"{fl}\")\n",
    "        napi.download_dataset(\n",
    "            os.path.join(dataset_name, fl),\n",
    "            dest_path=os.path.join(data_path, fl)\n",
    "        )\n",
    "\n",
    "    napi.download_dataset(\n",
    "        f\"{dataset_name}/live_int8.parquet\",\n",
    "        os.path.join(data_path, f\"{current_round}/live_int8.parquet\"),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d731610e",
   "metadata": {},
   "source": [
    "### 2. Load up training and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "61d0d00f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# read the feature metadata and get a feature set (or all the features)\n",
    "with open(os.path.join(data_path, \"features.json\"), \"r\") as f:\n",
    "    feature_metadata = json.load(f)\n",
    "\n",
    "# features = feature_metadata[\"feature_sets\"][\"small\"] # get the small feature set\n",
    "features = feature_metadata[\"feature_sets\"][feature_set_name]  # get the medium feature set\n",
    "target_cols = feature_metadata[\"targets\"]\n",
    "# read in just those features along with era and target columns\n",
    "read_columns = features + target_cols + [ERA_COL, DATA_TYPE_COL]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "53e7e66d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading minimal training data\n",
      "Reading training data ...\n",
      "Reading validation data ...\n",
      "\r"
     ]
    }
   ],
   "source": [
    "if not TO_TRAIN:\n",
    "    print(\"Not loading training data\")\n",
    "else:\n",
    "    print(\"Reading minimal training data\")\n",
    "    # note: sometimes when trying to read the downloaded data you get an error about invalid magic parquet bytes...\n",
    "    # if so, delete the file and rerun the napi.download_dataset to fix the corrupted file\n",
    "    print(\"Reading training data ...\")\n",
    "    training_data = dd.read_parquet(os.path.join(data_path, \"train_int8.parquet\"), columns=read_columns)\n",
    "    print(\"Reading validation data ...\")\n",
    "    validation_data = dd.read_parquet(os.path.join(data_path, \"validation_int8.parquet\"), columns=read_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "db54d270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading live data ...\n",
      "\r"
     ]
    }
   ],
   "source": [
    "print(\"Reading live data ...\")\n",
    "live_data = dd.read_parquet(os.path.join(data_path, f\"{current_round}/live_int8.parquet\"), columns=read_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "be4e1794",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_abating_unadaptable_weakfish</th>\n",
       "      <th>feature_ablest_mauritanian_elding</th>\n",
       "      <th>feature_acclimatisable_unfeigned_maghreb</th>\n",
       "      <th>feature_accommodable_crinite_cleft</th>\n",
       "      <th>feature_accretive_sorrier_skedaddle</th>\n",
       "      <th>feature_acetose_periotic_coronation</th>\n",
       "      <th>feature_additive_untrustworthy_hierologist</th>\n",
       "      <th>feature_adsorbed_blizzardy_burlesque</th>\n",
       "      <th>feature_affettuoso_taxidermic_greg</th>\n",
       "      <th>feature_afoul_valvate_faery</th>\n",
       "      <th>...</th>\n",
       "      <th>target_george_v4_20</th>\n",
       "      <th>target_george_v4_60</th>\n",
       "      <th>target_william_v4_20</th>\n",
       "      <th>target_william_v4_60</th>\n",
       "      <th>target_arthur_v4_20</th>\n",
       "      <th>target_arthur_v4_60</th>\n",
       "      <th>target_thomas_v4_20</th>\n",
       "      <th>target_thomas_v4_60</th>\n",
       "      <th>era</th>\n",
       "      <th>data_type</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>n003bba8a98662e4</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0001</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n003bee128c2fcfc</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0001</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 672 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  feature_abating_unadaptable_weakfish   \n",
       "id                                                       \n",
       "n003bba8a98662e4                                     0  \\\n",
       "n003bee128c2fcfc                                     4   \n",
       "\n",
       "                  feature_ablest_mauritanian_elding   \n",
       "id                                                    \n",
       "n003bba8a98662e4                                  4  \\\n",
       "n003bee128c2fcfc                                  2   \n",
       "\n",
       "                  feature_acclimatisable_unfeigned_maghreb   \n",
       "id                                                           \n",
       "n003bba8a98662e4                                         0  \\\n",
       "n003bee128c2fcfc                                         2   \n",
       "\n",
       "                  feature_accommodable_crinite_cleft   \n",
       "id                                                     \n",
       "n003bba8a98662e4                                   4  \\\n",
       "n003bee128c2fcfc                                   2   \n",
       "\n",
       "                  feature_accretive_sorrier_skedaddle   \n",
       "id                                                      \n",
       "n003bba8a98662e4                                 <NA>  \\\n",
       "n003bee128c2fcfc                                 <NA>   \n",
       "\n",
       "                  feature_acetose_periotic_coronation   \n",
       "id                                                      \n",
       "n003bba8a98662e4                                    0  \\\n",
       "n003bee128c2fcfc                                    3   \n",
       "\n",
       "                  feature_additive_untrustworthy_hierologist   \n",
       "id                                                             \n",
       "n003bba8a98662e4                                           1  \\\n",
       "n003bee128c2fcfc                                           1   \n",
       "\n",
       "                  feature_adsorbed_blizzardy_burlesque   \n",
       "id                                                       \n",
       "n003bba8a98662e4                                     4  \\\n",
       "n003bee128c2fcfc                                     3   \n",
       "\n",
       "                  feature_affettuoso_taxidermic_greg   \n",
       "id                                                     \n",
       "n003bba8a98662e4                                   0  \\\n",
       "n003bee128c2fcfc                                   2   \n",
       "\n",
       "                  feature_afoul_valvate_faery  ...  target_george_v4_20   \n",
       "id                                             ...                        \n",
       "n003bba8a98662e4                            3  ...                 0.25  \\\n",
       "n003bee128c2fcfc                            2  ...                 1.00   \n",
       "\n",
       "                  target_george_v4_60  target_william_v4_20   \n",
       "id                                                            \n",
       "n003bba8a98662e4                  0.0              0.333333  \\\n",
       "n003bee128c2fcfc                  1.0              0.666667   \n",
       "\n",
       "                  target_william_v4_60  target_arthur_v4_20   \n",
       "id                                                            \n",
       "n003bba8a98662e4              0.000000             0.500000  \\\n",
       "n003bee128c2fcfc              0.666667             0.833333   \n",
       "\n",
       "                  target_arthur_v4_60  target_thomas_v4_20   \n",
       "id                                                           \n",
       "n003bba8a98662e4             0.500000             0.166667  \\\n",
       "n003bee128c2fcfc             0.666667             0.833333   \n",
       "\n",
       "                  target_thomas_v4_60   era  data_type  \n",
       "id                                                      \n",
       "n003bba8a98662e4             0.000000  0001      train  \n",
       "n003bee128c2fcfc             0.666667  0001      train  \n",
       "\n",
       "[2 rows x 672 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_abating_unadaptable_weakfish</th>\n",
       "      <th>feature_ablest_mauritanian_elding</th>\n",
       "      <th>feature_acclimatisable_unfeigned_maghreb</th>\n",
       "      <th>feature_accommodable_crinite_cleft</th>\n",
       "      <th>feature_accretive_sorrier_skedaddle</th>\n",
       "      <th>feature_acetose_periotic_coronation</th>\n",
       "      <th>feature_additive_untrustworthy_hierologist</th>\n",
       "      <th>feature_adsorbed_blizzardy_burlesque</th>\n",
       "      <th>feature_affettuoso_taxidermic_greg</th>\n",
       "      <th>feature_afoul_valvate_faery</th>\n",
       "      <th>...</th>\n",
       "      <th>target_george_v4_20</th>\n",
       "      <th>target_george_v4_60</th>\n",
       "      <th>target_william_v4_20</th>\n",
       "      <th>target_william_v4_60</th>\n",
       "      <th>target_arthur_v4_20</th>\n",
       "      <th>target_arthur_v4_60</th>\n",
       "      <th>target_thomas_v4_20</th>\n",
       "      <th>target_thomas_v4_60</th>\n",
       "      <th>era</th>\n",
       "      <th>data_type</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>n000101811a8a843</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0575</td>\n",
       "      <td>validation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n001e1318d5072ac</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0575</td>\n",
       "      <td>validation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 672 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  feature_abating_unadaptable_weakfish   \n",
       "id                                                       \n",
       "n000101811a8a843                                     0  \\\n",
       "n001e1318d5072ac                                     1   \n",
       "\n",
       "                  feature_ablest_mauritanian_elding   \n",
       "id                                                    \n",
       "n000101811a8a843                                  0  \\\n",
       "n001e1318d5072ac                                  4   \n",
       "\n",
       "                  feature_acclimatisable_unfeigned_maghreb   \n",
       "id                                                           \n",
       "n000101811a8a843                                         1  \\\n",
       "n001e1318d5072ac                                         3   \n",
       "\n",
       "                  feature_accommodable_crinite_cleft   \n",
       "id                                                     \n",
       "n000101811a8a843                                   0  \\\n",
       "n001e1318d5072ac                                   1   \n",
       "\n",
       "                  feature_accretive_sorrier_skedaddle   \n",
       "id                                                      \n",
       "n000101811a8a843                                    3  \\\n",
       "n001e1318d5072ac                                    3   \n",
       "\n",
       "                  feature_acetose_periotic_coronation   \n",
       "id                                                      \n",
       "n000101811a8a843                                    1  \\\n",
       "n001e1318d5072ac                                    1   \n",
       "\n",
       "                  feature_additive_untrustworthy_hierologist   \n",
       "id                                                             \n",
       "n000101811a8a843                                           0  \\\n",
       "n001e1318d5072ac                                           4   \n",
       "\n",
       "                  feature_adsorbed_blizzardy_burlesque   \n",
       "id                                                       \n",
       "n000101811a8a843                                     0  \\\n",
       "n001e1318d5072ac                                     4   \n",
       "\n",
       "                  feature_affettuoso_taxidermic_greg   \n",
       "id                                                     \n",
       "n000101811a8a843                                   1  \\\n",
       "n001e1318d5072ac                                   3   \n",
       "\n",
       "                  feature_afoul_valvate_faery  ...  target_george_v4_20   \n",
       "id                                             ...                        \n",
       "n000101811a8a843                            2  ...                  0.5  \\\n",
       "n001e1318d5072ac                            1  ...                  0.0   \n",
       "\n",
       "                  target_george_v4_60  target_william_v4_20   \n",
       "id                                                            \n",
       "n000101811a8a843                  0.5              0.666667  \\\n",
       "n001e1318d5072ac                  0.5              0.166667   \n",
       "\n",
       "                  target_william_v4_60  target_arthur_v4_20   \n",
       "id                                                            \n",
       "n000101811a8a843              0.500000             0.500000  \\\n",
       "n001e1318d5072ac              0.333333             0.333333   \n",
       "\n",
       "                  target_arthur_v4_60  target_thomas_v4_20   \n",
       "id                                                           \n",
       "n000101811a8a843             0.500000             0.666667  \\\n",
       "n001e1318d5072ac             0.166667             0.166667   \n",
       "\n",
       "                  target_thomas_v4_60   era   data_type  \n",
       "id                                                       \n",
       "n000101811a8a843             0.500000  0575  validation  \n",
       "n001e1318d5072ac             0.333333  0575  validation  \n",
       "\n",
       "[2 rows x 672 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_abating_unadaptable_weakfish</th>\n",
       "      <th>feature_ablest_mauritanian_elding</th>\n",
       "      <th>feature_acclimatisable_unfeigned_maghreb</th>\n",
       "      <th>feature_accommodable_crinite_cleft</th>\n",
       "      <th>feature_accretive_sorrier_skedaddle</th>\n",
       "      <th>feature_acetose_periotic_coronation</th>\n",
       "      <th>feature_additive_untrustworthy_hierologist</th>\n",
       "      <th>feature_adsorbed_blizzardy_burlesque</th>\n",
       "      <th>feature_affettuoso_taxidermic_greg</th>\n",
       "      <th>feature_afoul_valvate_faery</th>\n",
       "      <th>...</th>\n",
       "      <th>target_george_v4_20</th>\n",
       "      <th>target_george_v4_60</th>\n",
       "      <th>target_william_v4_20</th>\n",
       "      <th>target_william_v4_60</th>\n",
       "      <th>target_arthur_v4_20</th>\n",
       "      <th>target_arthur_v4_60</th>\n",
       "      <th>target_thomas_v4_20</th>\n",
       "      <th>target_thomas_v4_60</th>\n",
       "      <th>era</th>\n",
       "      <th>data_type</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>n001b3b98ab6bc91</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>X</td>\n",
       "      <td>live</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n001eb0ac2f58573</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>X</td>\n",
       "      <td>live</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 672 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  feature_abating_unadaptable_weakfish   \n",
       "id                                                       \n",
       "n001b3b98ab6bc91                                     0  \\\n",
       "n001eb0ac2f58573                                     1   \n",
       "\n",
       "                  feature_ablest_mauritanian_elding   \n",
       "id                                                    \n",
       "n001b3b98ab6bc91                                  0  \\\n",
       "n001eb0ac2f58573                                  0   \n",
       "\n",
       "                  feature_acclimatisable_unfeigned_maghreb   \n",
       "id                                                           \n",
       "n001b3b98ab6bc91                                         1  \\\n",
       "n001eb0ac2f58573                                         2   \n",
       "\n",
       "                  feature_accommodable_crinite_cleft   \n",
       "id                                                     \n",
       "n001b3b98ab6bc91                                   4  \\\n",
       "n001eb0ac2f58573                                   4   \n",
       "\n",
       "                  feature_accretive_sorrier_skedaddle   \n",
       "id                                                      \n",
       "n001b3b98ab6bc91                                    1  \\\n",
       "n001eb0ac2f58573                                    2   \n",
       "\n",
       "                  feature_acetose_periotic_coronation   \n",
       "id                                                      \n",
       "n001b3b98ab6bc91                                    0  \\\n",
       "n001eb0ac2f58573                                    4   \n",
       "\n",
       "                  feature_additive_untrustworthy_hierologist   \n",
       "id                                                             \n",
       "n001b3b98ab6bc91                                           2  \\\n",
       "n001eb0ac2f58573                                           3   \n",
       "\n",
       "                  feature_adsorbed_blizzardy_burlesque   \n",
       "id                                                       \n",
       "n001b3b98ab6bc91                                     2  \\\n",
       "n001eb0ac2f58573                                     3   \n",
       "\n",
       "                  feature_affettuoso_taxidermic_greg   \n",
       "id                                                     \n",
       "n001b3b98ab6bc91                                   1  \\\n",
       "n001eb0ac2f58573                                   2   \n",
       "\n",
       "                  feature_afoul_valvate_faery  ...  target_george_v4_20   \n",
       "id                                             ...                        \n",
       "n001b3b98ab6bc91                            0  ...                  NaN  \\\n",
       "n001eb0ac2f58573                            1  ...                  NaN   \n",
       "\n",
       "                  target_george_v4_60  target_william_v4_20   \n",
       "id                                                            \n",
       "n001b3b98ab6bc91                  NaN                   NaN  \\\n",
       "n001eb0ac2f58573                  NaN                   NaN   \n",
       "\n",
       "                  target_william_v4_60  target_arthur_v4_20   \n",
       "id                                                            \n",
       "n001b3b98ab6bc91                   NaN                  NaN  \\\n",
       "n001eb0ac2f58573                   NaN                  NaN   \n",
       "\n",
       "                  target_arthur_v4_60  target_thomas_v4_20   \n",
       "id                                                           \n",
       "n001b3b98ab6bc91                  NaN                  NaN  \\\n",
       "n001eb0ac2f58573                  NaN                  NaN   \n",
       "\n",
       "                  target_thomas_v4_60  era  data_type  \n",
       "id                                                     \n",
       "n001b3b98ab6bc91                  NaN    X       live  \n",
       "n001eb0ac2f58573                  NaN    X       live  \n",
       "\n",
       "[2 rows x 672 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "if TO_TRAIN:\n",
    "    display(training_data.head(2))\n",
    "    display(validation_data.head(2))\n",
    "display(live_data.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9419d5",
   "metadata": {},
   "source": [
    "### 3. Subsample training and validation data and store in single DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "912c7e11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subsampling every fourth era...\n",
      "Subsampling every fourth era...\n",
      "\r"
     ]
    }
   ],
   "source": [
    "if TO_TRAIN:\n",
    "    # features = list(feature_metadata[\"feature_stats\"].keys()) # get all the features\n",
    "    # reduce the number of eras to every 4th era to speed things up... uncomment these lines to speed things up.\n",
    "    if TRAINING_PARAMS[\"sample_every4\"]:\n",
    "        print(\"Subsampling every fourth era...\")\n",
    "        every_4th_era = set(training_data[ERA_COL].unique()[::4])\n",
    "        training_data = training_data[training_data[ERA_COL].isin(every_4th_era)]\n",
    "        every_4th_era = set(validation_data[ERA_COL].unique()[::4])\n",
    "        validation_data = validation_data[validation_data[ERA_COL].isin(every_4th_era)]\n",
    "\n",
    "\n",
    "        print(\"Subsampling every fourth era...\")\n",
    "    # get all the data to possibly use for training\n",
    "    all_data = dd.concat([training_data, validation_data])\n",
    "\n",
    "    # save indices for easier data selection later\n",
    "    training_index = training_data.index.compute()\n",
    "    validation_index = validation_data.index.compute()\n",
    "    all_index = all_data.index.compute()\n",
    "\n",
    "    # delete training and validation data to save space\n",
    "    del training_data\n",
    "    del validation_data\n",
    "    gc.collect()  # clear up memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e9371adb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h5>all_ixes</h5>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "era_col.min()=dd.Scalar<series-..., type=str>, era_col.max()=dd.Scalar<series-..., type=str>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><strong>Dask DataFrame Structure:</strong></div>\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>era</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>npartitions=1</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "<div>Dask Name: to_frame, 23 graph layers</div>"
      ],
      "text/plain": [
       "Dask DataFrame Structure:\n",
       "                  era\n",
       "npartitions=1        \n",
       "               object\n",
       "                  ...\n",
       "Dask Name: to_frame, 23 graph layers"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h5>train_ixes</h5>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "era_col.min()=dd.Scalar<series-..., type=str>, era_col.max()=dd.Scalar<series-..., type=str>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><strong>Dask DataFrame Structure:</strong></div>\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>era</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>npartitions=1</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "<div>Dask Name: to_frame, 23 graph layers</div>"
      ],
      "text/plain": [
       "Dask DataFrame Structure:\n",
       "                  era\n",
       "npartitions=1        \n",
       "               object\n",
       "                  ...\n",
       "Dask Name: to_frame, 23 graph layers"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h5>val_ixes</h5>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "era_col.min()=dd.Scalar<series-..., type=str>, era_col.max()=dd.Scalar<series-..., type=str>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><strong>Dask DataFrame Structure:</strong></div>\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>era</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>npartitions=1</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "<div>Dask Name: to_frame, 23 graph layers</div>"
      ],
      "text/plain": [
       "Dask DataFrame Structure:\n",
       "                  era\n",
       "npartitions=1        \n",
       "               object\n",
       "                  ...\n",
       "Dask Name: to_frame, 23 graph layers"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "if TO_TRAIN:\n",
    "    for name, ixes in [(\"all_ixes\", all_index), (\"train_ixes\", training_index), (\"val_ixes\", validation_index)]:\n",
    "        era_col = all_data.loc[ixes, ERA_COL]\n",
    "        display(HTML(f\"<h5>{name}</h5>\"))\n",
    "        print(f\"{era_col.min()=}, {era_col.max()=}\")\n",
    "        display(era_col.describe().to_frame())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1d9c8c",
   "metadata": {},
   "source": [
    "### 4. Impute NAs with median values as int8 cannot handle NAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ccddc41f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaning up NAs in live data...\n",
      "cleaning up NAs in train and validation data...\n",
      "\r"
     ]
    }
   ],
   "source": [
    "print(\"cleaning up NAs in live data...\")\n",
    "live_data[features] = live_data[features].fillna(0.5)  # since live data is only one era, we need to use the median for all eras\n",
    "live_data[features] = live_data[features].astype(\"int8\")  # make sure change to float32 if using the non int8 data!\n",
    "# Alternatively could convert nan columns to be floats and replace pd.NA with np.nan\n",
    "\n",
    "if TO_TRAIN:\n",
    "    print(\"cleaning up NAs in train and validation data...\")\n",
    "    # Int8 datatype has pd.NA which don't play nice with models.  We simply fill NA with median values here\n",
    "    all_data[features] = all_data[features].fillna(0.5)\n",
    "    all_data[features] = all_data[features].astype(\"int8\")  # make sure change to float32 if using the non int8 data!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558ca9b0",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f538cbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if TO_TRAIN:\n",
    "    mlflow.start_run(run_name=RUN_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5986d601",
   "metadata": {},
   "source": [
    "### 1. Fast train the model with different targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "23465fa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training models and saving them...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e643b06049c44d4080b3c30bf423e1b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for existing model 'train_data_sm_lgbm_v4.1_medium_target_nomi_v4_20'\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Cannot interpret 'Int8Dtype()' as a data type",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [37]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m     save_model(train_model, train_data_model_name)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# predict on validation data\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m all_data\u001b[38;5;241m.\u001b[39mloc[validation_index, prediction_col] \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mall_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mvalidation_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m gc\u001b[38;5;241m.\u001b[39mcollect()\n",
      "File \u001b[0;32m~/venvs/venv3/lib/python3.10/site-packages/lightgbm/sklearn.py:797\u001b[0m, in \u001b[0;36mLGBMModel.predict\u001b[0;34m(self, X, raw_score, start_iteration, num_iteration, pred_leaf, pred_contrib, **kwargs)\u001b[0m\n\u001b[1;32m    795\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LGBMNotFittedError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEstimator not fitted, call fit before exploiting the model.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    796\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(X, (pd_DataFrame, dt_DataTable)):\n\u001b[0;32m--> 797\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[43m_LGBMCheckArray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    798\u001b[0m n_features \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    799\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_features \u001b[38;5;241m!=\u001b[39m n_features:\n",
      "File \u001b[0;32m~/venvs/venv3/lib/python3.10/site-packages/sklearn/utils/validation.py:856\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    854\u001b[0m         array \u001b[38;5;241m=\u001b[39m array\u001b[38;5;241m.\u001b[39mastype(dtype, casting\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munsafe\u001b[39m\u001b[38;5;124m\"\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    855\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 856\u001b[0m         array \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ComplexWarning \u001b[38;5;28;01mas\u001b[39;00m complex_warning:\n\u001b[1;32m    858\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    859\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComplex data not supported\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(array)\n\u001b[1;32m    860\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcomplex_warning\u001b[39;00m\n",
      "File \u001b[0;32m~/venvs/venv3/lib/python3.10/site-packages/dask/dataframe/core.py:597\u001b[0m, in \u001b[0;36m_Frame.__array__\u001b[0;34m(self, dtype, **kwargs)\u001b[0m\n\u001b[1;32m    596\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__array__\u001b[39m(\u001b[38;5;28mself\u001b[39m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 597\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_computed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    598\u001b[0m     x \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_computed)\n\u001b[1;32m    599\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/venvs/venv3/lib/python3.10/site-packages/dask/base.py:314\u001b[0m, in \u001b[0;36mDaskMethodsMixin.compute\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    291\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute this dask collection\u001b[39;00m\n\u001b[1;32m    292\u001b[0m \n\u001b[1;32m    293\u001b[0m \u001b[38;5;124;03m    This turns a lazy Dask collection into its in-memory equivalent.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;124;03m    dask.base.compute\u001b[39;00m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 314\u001b[0m     (result,) \u001b[38;5;241m=\u001b[39m \u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraverse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    315\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/venvs/venv3/lib/python3.10/site-packages/dask/base.py:599\u001b[0m, in \u001b[0;36mcompute\u001b[0;34m(traverse, optimize_graph, scheduler, get, *args, **kwargs)\u001b[0m\n\u001b[1;32m    596\u001b[0m     keys\u001b[38;5;241m.\u001b[39mappend(x\u001b[38;5;241m.\u001b[39m__dask_keys__())\n\u001b[1;32m    597\u001b[0m     postcomputes\u001b[38;5;241m.\u001b[39mappend(x\u001b[38;5;241m.\u001b[39m__dask_postcompute__())\n\u001b[0;32m--> 599\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mschedule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdsk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    600\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m repack([f(r, \u001b[38;5;241m*\u001b[39ma) \u001b[38;5;28;01mfor\u001b[39;00m r, (f, a) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(results, postcomputes)])\n",
      "File \u001b[0;32m~/venvs/venv3/lib/python3.10/site-packages/dask/threaded.py:89\u001b[0m, in \u001b[0;36mget\u001b[0;34m(dsk, keys, cache, num_workers, pool, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(pool, multiprocessing\u001b[38;5;241m.\u001b[39mpool\u001b[38;5;241m.\u001b[39mPool):\n\u001b[1;32m     87\u001b[0m         pool \u001b[38;5;241m=\u001b[39m MultiprocessingPoolExecutor(pool)\n\u001b[0;32m---> 89\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mget_async\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubmit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_max_workers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdsk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[43m    \u001b[49m\u001b[43mget_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_thread_get_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpack_exception\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpack_exception\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;66;03m# Cleanup pools associated to dead threads\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m pools_lock:\n",
      "File \u001b[0;32m~/venvs/venv3/lib/python3.10/site-packages/dask/local.py:511\u001b[0m, in \u001b[0;36mget_async\u001b[0;34m(submit, num_workers, dsk, result, cache, get_id, rerun_exceptions_locally, pack_exception, raise_exception, callbacks, dumps, loads, chunksize, **kwargs)\u001b[0m\n\u001b[1;32m    509\u001b[0m         _execute_task(task, data)  \u001b[38;5;66;03m# Re-execute locally\u001b[39;00m\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         \u001b[43mraise_exception\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    512\u001b[0m res, worker_id \u001b[38;5;241m=\u001b[39m loads(res_info)\n\u001b[1;32m    513\u001b[0m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcache\u001b[39m\u001b[38;5;124m\"\u001b[39m][key] \u001b[38;5;241m=\u001b[39m res\n",
      "File \u001b[0;32m~/venvs/venv3/lib/python3.10/site-packages/dask/local.py:319\u001b[0m, in \u001b[0;36mreraise\u001b[0;34m(exc, tb)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m exc\u001b[38;5;241m.\u001b[39m__traceback__ \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tb:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mwith_traceback(tb)\n\u001b[0;32m--> 319\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[0;32m~/venvs/venv3/lib/python3.10/site-packages/dask/local.py:224\u001b[0m, in \u001b[0;36mexecute_task\u001b[0;34m(key, task_info, dumps, loads, get_id, pack_exception)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    223\u001b[0m     task, data \u001b[38;5;241m=\u001b[39m loads(task_info)\n\u001b[0;32m--> 224\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_execute_task\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    225\u001b[0m     \u001b[38;5;28mid\u001b[39m \u001b[38;5;241m=\u001b[39m get_id()\n\u001b[1;32m    226\u001b[0m     result \u001b[38;5;241m=\u001b[39m dumps((result, \u001b[38;5;28mid\u001b[39m))\n",
      "File \u001b[0;32m~/venvs/venv3/lib/python3.10/site-packages/dask/core.py:119\u001b[0m, in \u001b[0;36m_execute_task\u001b[0;34m(arg, cache, dsk)\u001b[0m\n\u001b[1;32m    115\u001b[0m     func, args \u001b[38;5;241m=\u001b[39m arg[\u001b[38;5;241m0\u001b[39m], arg[\u001b[38;5;241m1\u001b[39m:]\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# Note: Don't assign the subtask results to a variable. numpy detects\u001b[39;00m\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;66;03m# temporaries by their reference count and can execute certain\u001b[39;00m\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;66;03m# operations in-place.\u001b[39;00m\n\u001b[0;32m--> 119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_execute_task\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ishashable(arg):\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arg\n",
      "File \u001b[0;32m~/venvs/venv3/lib/python3.10/site-packages/dask/utils.py:642\u001b[0m, in \u001b[0;36mDispatch.__call__\u001b[0;34m(self, arg, *args, **kwargs)\u001b[0m\n\u001b[1;32m    638\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    639\u001b[0m \u001b[38;5;124;03mCall the corresponding method based on type of argument.\u001b[39;00m\n\u001b[1;32m    640\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    641\u001b[0m meth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch(\u001b[38;5;28mtype\u001b[39m(arg))\n\u001b[0;32m--> 642\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmeth\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venvs/venv3/lib/python3.10/site-packages/dask/dataframe/backends.py:700\u001b[0m, in \u001b[0;36mpercentile\u001b[0;34m(a, q, interpolation)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;129m@percentile_lookup\u001b[39m\u001b[38;5;241m.\u001b[39mregister((pd\u001b[38;5;241m.\u001b[39mSeries, pd\u001b[38;5;241m.\u001b[39mIndex))\n\u001b[1;32m    699\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpercentile\u001b[39m(a, q, interpolation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlinear\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 700\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_percentile\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venvs/venv3/lib/python3.10/site-packages/dask/array/percentile.py:34\u001b[0m, in \u001b[0;36m_percentile\u001b[0;34m(a, q, method)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(a, (pd\u001b[38;5;241m.\u001b[39mSeries, pd\u001b[38;5;241m.\u001b[39mIndex)):\n\u001b[1;32m     32\u001b[0m         a \u001b[38;5;241m=\u001b[39m a\u001b[38;5;241m.\u001b[39mvalues\n\u001b[0;32m---> 34\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43missubdtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdatetime64\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     35\u001b[0m     values \u001b[38;5;241m=\u001b[39m a\n\u001b[1;32m     36\u001b[0m     a2 \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mi8\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/venvs/venv3/lib/python3.10/site-packages/numpy/core/numerictypes.py:416\u001b[0m, in \u001b[0;36missubdtype\u001b[0;34m(arg1, arg2)\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    359\u001b[0m \u001b[38;5;124;03mReturns True if first argument is a typecode lower/equal in type hierarchy.\u001b[39;00m\n\u001b[1;32m    360\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    413\u001b[0m \n\u001b[1;32m    414\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m issubclass_(arg1, generic):\n\u001b[0;32m--> 416\u001b[0m     arg1 \u001b[38;5;241m=\u001b[39m \u001b[43mdtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg1\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtype\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m issubclass_(arg2, generic):\n\u001b[1;32m    418\u001b[0m     arg2 \u001b[38;5;241m=\u001b[39m dtype(arg2)\u001b[38;5;241m.\u001b[39mtype\n",
      "\u001b[0;31mTypeError\u001b[0m: Cannot interpret 'Int8Dtype()' as a data type"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "if not TO_TRAIN:\n",
    "    print(\"Not training models...\")\n",
    "else:\n",
    "    print(\"Training models and saving them...\")\n",
    "    mlflow.log_params(params)\n",
    "    mlflow.log_param(\"targets\", targets)\n",
    "    mlflow.log_params(TRAINING_PARAMS)\n",
    "    for target in tqdm(targets):\n",
    "        prediction_col = f\"{params_name}_{dataset_name}_{feature_set_name}_{target}\"\n",
    "        train_data_model_name = f\"train_data_{prediction_col}\"\n",
    "        print(f\"Checking for existing model '{train_data_model_name}'\")\n",
    "        train_model = load_model(train_data_model_name)\n",
    "        if not train_model:\n",
    "            print(f\"model not found, creating new one\")\n",
    "            train_model = LGBMRegressor(**params)\n",
    "            # train on all of train and save the model so we don't have to train next time\n",
    "            target_train_index = (\n",
    "                all_data.loc[training_index, target].dropna().index\n",
    "            )  # make sure we only train on rows which have this target\n",
    "            train_model.fit(\n",
    "                all_data.loc[target_train_index, features],\n",
    "                all_data.loc[target_train_index, target],\n",
    "            )  # in case some of the targets are missing data\n",
    "            print(f\"saving new model: {train_data_model_name}\")\n",
    "            save_model(train_model, train_data_model_name)\n",
    "\n",
    "        # predict on validation data\n",
    "        all_data.loc[validation_index, prediction_col] = train_model.predict(\n",
    "            all_data.loc[validation_index, features]\n",
    "        )\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1cf98e99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "def get_pred_col_name(target, params_name, dataset_name, feature_set_name):\n",
    "    return f\"{params_name}_{dataset_name}_{feature_set_name}_{target}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c4ec464c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed4efe955a9948bdb99dfbda70667fe2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for existing model 'all_data_sm_lgbm_v4.1_medium_target_nomi_v4_20'\n",
      "Checking for existing model 'all_data_sm_lgbm_v4.1_medium_target_jerome_v4_60'\n",
      "Checking for existing model 'all_data_sm_lgbm_v4.1_medium_target_ralph_v4_20'\n",
      "Checking for existing model 'all_data_sm_lgbm_v4.1_medium_target_tyler_v4_20'\n",
      "Checking for existing model 'all_data_sm_lgbm_v4.1_medium_target_victor_v4_20'\n",
      "Checking for existing model 'all_data_sm_lgbm_v4.1_medium_target_waldo_v4_20'\n",
      "\r"
     ]
    }
   ],
   "source": [
    "for target in tqdm(targets):\n",
    "    prediction_col = get_pred_col_name(target, params_name, dataset_name, feature_set_name)\n",
    "    # do the same thing for all data (for predicting on live)\n",
    "    all_data_model_name = f\"all_data_{prediction_col}\"\n",
    "    print(f\"Checking for existing model '{all_data_model_name}'\")\n",
    "    all_data_model = load_model(all_data_model_name)\n",
    "    if not all_data_model:\n",
    "        print(f\"model not found, creating new one\")\n",
    "        raise ValueError(\"Model is not trained and saved, switch to TO_TRAIN mode and train the model.\")\n",
    "    # predict on live data\n",
    "    live_data[prediction_col] = all_data_model.predict(\n",
    "        live_data[features].fillna(np.nan)\n",
    "    )  # filling live data with nans makes us ignore those features if necessary\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbced23",
   "metadata": {},
   "source": [
    "### 2. Equal weight the different targets and then neutralise 50% of the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c061274d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "def get_raw_pred_cols():\n",
    "    \"\"\"These are the columns of the output dataframe where predicted values are stored\"\"\"\n",
    "    return [\n",
    "        get_pred_col_name(\n",
    "            target=tgt,\n",
    "            params_name=params_name,\n",
    "            dataset_name=dataset_name,\n",
    "            feature_set_name=feature_set_name,\n",
    "        )\n",
    "        for tgt in targets\n",
    "    ]\n",
    "\n",
    "\n",
    "def get_all_pred_cols():\n",
    "    \"\"\"These include the prediction columns in :meth:`get_raw_pred_cols` but\n",
    "    also derived columns from neutralisation.\n",
    "    \"\"\"\n",
    "    return (\n",
    "        [\"equal_weight\", \"half_neutral_equal_weight\"]\n",
    "        + get_pred_col_name(\n",
    "            targets=targets,\n",
    "            params_name=params_name,\n",
    "            dataset_name=dataset_name,\n",
    "            feature_set_name=feature_set_name,\n",
    "        )\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b07f0ddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yb/bysg0gmj6sdb_006hzx07fjc0000gn/T/ipykernel_53737/2930966495.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[:, \"equal_weight\"] = df[get_raw_pred_cols()].mean(axis=1)\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  2.01it/s]\n",
      "/var/folders/yb/bysg0gmj6sdb_006hzx07fjc0000gn/T/ipykernel_53737/2930966495.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"half_neutral_equal_weight\"] = neutralize(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "model_to_submit = f\"half_neutral_equal_weight\"\n",
    "if TO_TRAIN:\n",
    "    data_w_ixes = [(live_data, live_data.index), (all_data, validation_index)]\n",
    "else:\n",
    "    data_w_ixes = [(live_data, live_data.index)]\n",
    "\n",
    "for df, ixes in data_w_ixes:\n",
    "    # make an ensemble\n",
    "    df.loc[:, \"equal_weight\"] = df[get_raw_pred_cols()].mean(axis=1)\n",
    "    # make a 50% feature neutral variation of the ensemble model\n",
    "    df[\"half_neutral_equal_weight\"] = neutralize(\n",
    "        df=df.loc[ixes, :],\n",
    "        columns=[f\"equal_weight\"],\n",
    "        neutralizers=features,\n",
    "        proportion=TRAINING_PARAMS[\"neutralisation\"],\n",
    "        normalize=True,\n",
    "        era_col=ERA_COL,\n",
    "        verbose=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071b84b9",
   "metadata": {},
   "source": [
    "### 3. Make the predictions submission ready for numer.ai website\n",
    "\n",
    "Convert regressed values to rank values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2055f179",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yb/bysg0gmj6sdb_006hzx07fjc0000gn/T/ipykernel_53737/2078002967.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  all_data.loc[validation_index, \"prediction\"] = all_data.loc[\n",
      "/var/folders/yb/bysg0gmj6sdb_006hzx07fjc0000gn/T/ipykernel_53737/2078002967.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  live_data[\"prediction\"] = live_data[model_to_submit].rank(pct=True)\n",
      "/var/folders/yb/bysg0gmj6sdb_006hzx07fjc0000gn/T/ipykernel_53737/2078002967.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  all_data.loc[validation_index, EXAMPLE_PREDS_COL] = validation_example_preds[\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# rename best model to \"prediction\" and rank from 0 to 1 to meet upload requirements\n",
    "if TO_TRAIN:\n",
    "    print(\"rename best model to 'prediction' and rank from 0 to 1 to meet upload requirements\")\n",
    "    all_data.loc[validation_index, \"prediction\"] = all_data.loc[\n",
    "        validation_index, model_to_submit\n",
    "    ].rank(pct=True)\n",
    "    all_data.loc[validation_index, \"prediction\"].to_csv(\n",
    "        f\"validation_predictions_{current_round}.csv\"\n",
    "    )\n",
    "    validation_example_preds = pd.read_parquet(\n",
    "        os.path.join(data_path, f\"validation_example_preds.parquet\"),\n",
    "    )\n",
    "    all_data.loc[validation_index, EXAMPLE_PREDS_COL] = validation_example_preds[\n",
    "        \"prediction\"\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a1923a9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yb/bysg0gmj6sdb_006hzx07fjc0000gn/T/ipykernel_53737/2935962428.py:1: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  live_data[\"prediction\"] = live_data[model_to_submit].rank(pct=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "live_data[\"prediction\"] = live_data[model_to_submit].rank(pct=True)\n",
    "live_data[\"prediction\"].to_csv(f\"live_predictions_{current_round}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "093e5a92",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/vispers/work/numerai/numerai'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "73aafbfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__init__.py                     mlruns/\n",
      "__pycache__/                    modelling-sunshine.ipynb\n",
      "analysis_and_tips.ipynb         modelling-v1.ipynb\n",
      "data/                           modelling_utils.py\n",
      "download_numerai_dataset.py     models/\n",
      "example-model-advanced.ipynb    outputs/\n",
      "live_predictions_455.csv        utils.py\n",
      "live_predictions_458.csv        validation_predictions_455.csv\n",
      "metrics.py\n",
      "\r"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e2b0fa",
   "metadata": {},
   "source": [
    "### 4. Print some metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9e07e3f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|                                         |      mean |   sharpe |\n",
      "|:----------------------------------------|----------:|---------:|\n",
      "| sm_lgbm_v4.1_medium_target_nomi_v4_20   | 0.0257037 | 0.718577 |\n",
      "| sm_lgbm_v4.1_medium_target_jerome_v4_60 | 0.0235539 | 0.840822 |\n",
      "| sm_lgbm_v4.1_medium_target_ralph_v4_20  | 0.0264948 | 0.789863 |\n",
      "| sm_lgbm_v4.1_medium_target_tyler_v4_20  | 0.0235424 | 0.678123 |\n",
      "| sm_lgbm_v4.1_medium_target_victor_v4_20 | 0.0285276 | 0.942859 |\n",
      "| sm_lgbm_v4.1_medium_target_waldo_v4_20  | 0.0259589 | 0.790595 |\n",
      "| equal_weight                            | 0.0276816 | 0.812603 |\n",
      "| half_neutral_equal_weight               | 0.0287806 | 0.940548 |\n",
      "\n",
      "Done! Next steps:\n",
      "    1. Go to numer.ai/tournament (make sure you have an account)\n",
      "    2. Submit validation_predictions_455.csv to the diagnostics tool\n",
      "    3. Submit tournament_predictions_455.csv to the \"Upload Predictions\" button\n",
      "\n",
      "\r"
     ]
    }
   ],
   "source": [
    "if TO_TRAIN:\n",
    "    # get some stats about each of our models to compare...\n",
    "    # fast_mode=True so that we skip some of the stats that are slower to calculate\n",
    "    validation_stats = validation_metrics(\n",
    "        all_data.loc[validation_index, :],\n",
    "        prediction_cols,\n",
    "        example_col=EXAMPLE_PREDS_COL,\n",
    "        fast_mode=True,\n",
    "        target_col=TARGET_COL,\n",
    "    )\n",
    "    print(validation_stats[[\"mean\", \"sharpe\"]].to_markdown())\n",
    "\n",
    "    print(\n",
    "        f\"\"\"\n",
    "    Done! Next steps:\n",
    "        1. Go to numer.ai/tournament (make sure you have an account)\n",
    "        2. Submit validation_predictions_{current_round}.csv to the diagnostics tool\n",
    "        3. Submit tournament_predictions_{current_round}.csv to the \"Upload Predictions\" button\n",
    "    \"\"\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "949b720e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>sharpe</th>\n",
       "      <th>max_drawdown</th>\n",
       "      <th>apy</th>\n",
       "      <th>mmc_mean</th>\n",
       "      <th>corr_plus_mmc_sharpe</th>\n",
       "      <th>corr_with_example_preds</th>\n",
       "      <th>exposure_dissimilarity_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sm_lgbm_v4.1_medium_target_nomi_v4_20</th>\n",
       "      <td>0.025704</td>\n",
       "      <td>0.035770</td>\n",
       "      <td>0.718577</td>\n",
       "      <td>-0.097479</td>\n",
       "      <td>233.218168</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.720733</td>\n",
       "      <td>-0.598389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sm_lgbm_v4.1_medium_target_jerome_v4_60</th>\n",
       "      <td>0.023554</td>\n",
       "      <td>0.028013</td>\n",
       "      <td>0.840822</td>\n",
       "      <td>-0.041424</td>\n",
       "      <td>204.415494</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.604949</td>\n",
       "      <td>-0.496763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sm_lgbm_v4.1_medium_target_ralph_v4_20</th>\n",
       "      <td>0.026495</td>\n",
       "      <td>0.033544</td>\n",
       "      <td>0.789863</td>\n",
       "      <td>-0.086138</td>\n",
       "      <td>247.215804</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.707534</td>\n",
       "      <td>-0.666071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sm_lgbm_v4.1_medium_target_tyler_v4_20</th>\n",
       "      <td>0.023542</td>\n",
       "      <td>0.034717</td>\n",
       "      <td>0.678123</td>\n",
       "      <td>-0.097209</td>\n",
       "      <td>201.233089</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.674070</td>\n",
       "      <td>-0.731769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sm_lgbm_v4.1_medium_target_victor_v4_20</th>\n",
       "      <td>0.028528</td>\n",
       "      <td>0.030256</td>\n",
       "      <td>0.942859</td>\n",
       "      <td>-0.057966</td>\n",
       "      <td>284.167180</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.763170</td>\n",
       "      <td>-0.219079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sm_lgbm_v4.1_medium_target_waldo_v4_20</th>\n",
       "      <td>0.025959</td>\n",
       "      <td>0.032835</td>\n",
       "      <td>0.790595</td>\n",
       "      <td>-0.095604</td>\n",
       "      <td>238.877693</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.699111</td>\n",
       "      <td>-0.726325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>equal_weight</th>\n",
       "      <td>0.027682</td>\n",
       "      <td>0.034065</td>\n",
       "      <td>0.812603</td>\n",
       "      <td>-0.074339</td>\n",
       "      <td>267.003844</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.753576</td>\n",
       "      <td>-0.748392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>half_neutral_equal_weight</th>\n",
       "      <td>0.028781</td>\n",
       "      <td>0.030600</td>\n",
       "      <td>0.940548</td>\n",
       "      <td>-0.051664</td>\n",
       "      <td>288.595137</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.827405</td>\n",
       "      <td>-0.267703</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             mean       std    sharpe   \n",
       "sm_lgbm_v4.1_medium_target_nomi_v4_20    0.025704  0.035770  0.718577  \\\n",
       "sm_lgbm_v4.1_medium_target_jerome_v4_60  0.023554  0.028013  0.840822   \n",
       "sm_lgbm_v4.1_medium_target_ralph_v4_20   0.026495  0.033544  0.789863   \n",
       "sm_lgbm_v4.1_medium_target_tyler_v4_20   0.023542  0.034717  0.678123   \n",
       "sm_lgbm_v4.1_medium_target_victor_v4_20  0.028528  0.030256  0.942859   \n",
       "sm_lgbm_v4.1_medium_target_waldo_v4_20   0.025959  0.032835  0.790595   \n",
       "equal_weight                             0.027682  0.034065  0.812603   \n",
       "half_neutral_equal_weight                0.028781  0.030600  0.940548   \n",
       "\n",
       "                                         max_drawdown         apy  mmc_mean   \n",
       "sm_lgbm_v4.1_medium_target_nomi_v4_20       -0.097479  233.218168       NaN  \\\n",
       "sm_lgbm_v4.1_medium_target_jerome_v4_60     -0.041424  204.415494       NaN   \n",
       "sm_lgbm_v4.1_medium_target_ralph_v4_20      -0.086138  247.215804       NaN   \n",
       "sm_lgbm_v4.1_medium_target_tyler_v4_20      -0.097209  201.233089       NaN   \n",
       "sm_lgbm_v4.1_medium_target_victor_v4_20     -0.057966  284.167180       NaN   \n",
       "sm_lgbm_v4.1_medium_target_waldo_v4_20      -0.095604  238.877693       NaN   \n",
       "equal_weight                                -0.074339  267.003844       NaN   \n",
       "half_neutral_equal_weight                   -0.051664  288.595137       NaN   \n",
       "\n",
       "                                         corr_plus_mmc_sharpe   \n",
       "sm_lgbm_v4.1_medium_target_nomi_v4_20                     NaN  \\\n",
       "sm_lgbm_v4.1_medium_target_jerome_v4_60                   NaN   \n",
       "sm_lgbm_v4.1_medium_target_ralph_v4_20                    NaN   \n",
       "sm_lgbm_v4.1_medium_target_tyler_v4_20                    NaN   \n",
       "sm_lgbm_v4.1_medium_target_victor_v4_20                   NaN   \n",
       "sm_lgbm_v4.1_medium_target_waldo_v4_20                    NaN   \n",
       "equal_weight                                              NaN   \n",
       "half_neutral_equal_weight                                 NaN   \n",
       "\n",
       "                                         corr_with_example_preds   \n",
       "sm_lgbm_v4.1_medium_target_nomi_v4_20                   0.720733  \\\n",
       "sm_lgbm_v4.1_medium_target_jerome_v4_60                 0.604949   \n",
       "sm_lgbm_v4.1_medium_target_ralph_v4_20                  0.707534   \n",
       "sm_lgbm_v4.1_medium_target_tyler_v4_20                  0.674070   \n",
       "sm_lgbm_v4.1_medium_target_victor_v4_20                 0.763170   \n",
       "sm_lgbm_v4.1_medium_target_waldo_v4_20                  0.699111   \n",
       "equal_weight                                            0.753576   \n",
       "half_neutral_equal_weight                               0.827405   \n",
       "\n",
       "                                         exposure_dissimilarity_mean  \n",
       "sm_lgbm_v4.1_medium_target_nomi_v4_20                      -0.598389  \n",
       "sm_lgbm_v4.1_medium_target_jerome_v4_60                    -0.496763  \n",
       "sm_lgbm_v4.1_medium_target_ralph_v4_20                     -0.666071  \n",
       "sm_lgbm_v4.1_medium_target_tyler_v4_20                     -0.731769  \n",
       "sm_lgbm_v4.1_medium_target_victor_v4_20                    -0.219079  \n",
       "sm_lgbm_v4.1_medium_target_waldo_v4_20                     -0.726325  \n",
       "equal_weight                                               -0.748392  \n",
       "half_neutral_equal_weight                                  -0.267703  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "validation_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "be777aeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'f3827ac21c4248189889f3a6c6a36576'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "mlflow.active_run().info.run_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "aa6f7646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "out_folder = f\"outputs/{mlflow.active_run().info.run_id}/\"\n",
    "val_stats_html_path = os.path.join(out_folder, \"metrics.html\")\n",
    "os.makedirs(out_folder, exist_ok=True)\n",
    "validation_stats.to_html(buf=val_stats_html_path)\n",
    "mlflow.log_artifact(local_path=val_stats_html_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46b35b6",
   "metadata": {},
   "source": [
    "#### Log metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "88e99381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "for ix in validation_stats.index:\n",
    "    for col in validation_stats.columns:\n",
    "        mlflow.log_metric(f\"{col}__{ix}\", validation_stats.loc[ix, col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0869479a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>sharpe</th>\n",
       "      <th>max_drawdown</th>\n",
       "      <th>apy</th>\n",
       "      <th>mmc_mean</th>\n",
       "      <th>corr_plus_mmc_sharpe</th>\n",
       "      <th>corr_with_example_preds</th>\n",
       "      <th>exposure_dissimilarity_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>example_preds</th>\n",
       "      <td>0.032386</td>\n",
       "      <td>0.030466</td>\n",
       "      <td>1.063026</td>\n",
       "      <td>-0.044031</td>\n",
       "      <td>360.733121</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   mean       std    sharpe  max_drawdown         apy   \n",
       "example_preds  0.032386  0.030466  1.063026     -0.044031  360.733121  \\\n",
       "\n",
       "               mmc_mean  corr_plus_mmc_sharpe  corr_with_example_preds   \n",
       "example_preds       NaN                   NaN                      1.0  \\\n",
       "\n",
       "               exposure_dissimilarity_mean  \n",
       "example_preds                          0.0  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "validation_metrics(\n",
    "    validation_data=all_data.loc[validation_index, :],\n",
    "    pred_cols=[EXAMPLE_PREDS_COL],\n",
    "    example_col=EXAMPLE_PREDS_COL,\n",
    "    fast_mode=True,\n",
    "    target_col=TARGET_COL,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c754fb10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "validation_stats.to_html(\"validation_stats.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d3f88cc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000df8cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
